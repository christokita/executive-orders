# Load data
####################
load("data/eosCurated.RData")
##################################################################################################
#
# Topic model EOs
#
##################################################################################################
rm(list =ls())
rm(list =ls())
source("scripts/util/__Util_MASTER.R")
library(parallel)
library(snowfall)
####################
# Load data
####################
load("data/eosCurated.RData")
####################
# Topic model
####################
# Set topic numbers to test
Ks <- seq(10, 100, 5)
k = 85
# Save
file_name <- paste0("EO_lda_k", k)
file_name
# Save
file_name <- paste0("eo_lda_k", k)
file_name
Ks
####################
# Topic model
####################
# Set topic numbers to test
Ks <- seq(20, 110, 5)
Ks
rm(list =ls())
source("scripts/util/__Util_MASTER.R")
library(parallel)
library(snowfall)
stopwords('smart')
load("data_derived/dtms/eo_dtm.Rdata")
dim(eo_dt)
dim(eo_dtm)
hist(colSums(eo_dtm))
hist(colSums(eo_dtm), breaks = 1)
hist(colSums(eo_dtm), breaks = 1000)
word_freq <- colSums(eo_dtm)
word_freq <- log10(word_freq)
hist(word_freq)
table(word_freq)
# Basic corpus statistics
tf <- TermDocFreq(dtm = eo_dtm)
names(tf)
View(tf)
hist(tf$doc_freq)
load('data/eosCurated.RData')
tf$doc_freq_perc <- tf$doc_freq / nrow(eos)
jost)tf$doc_freq_perc
hist(tf$doc_freq_perc)
hist.data <- hist(tf$doc_freq_perc, plot = F)
hist.data$counts <- log10(hist.data$counts)
plot(hist.data)
qplot(hist.data)
str(hist.data)
hist(hist.data)
plot(hist.data)
tf$term[tf$doc_freq > 3000]
tf$term[tf$doc_freq > nrow(eos)/2]
tf$term[tf$doc_freq < nrow(eos)/100]
keep_terms <- tf$term[ tf$doc.freq <= nrow(dtm)/2 ]
keep_terms <- tf$term[ tf$doc.freq <= nrow(eo_dtm)/2 ]
keep_terms <- tf$term[ tf$doc_freq <= nrow(eo_dtm)/2 ]
eo_dtm <- eo_dtm[, keep_terms]
summary(rowSums(eo_dtm)) #check to make sure all documents have words
which(rowSums(eo_dtm == 2))
which(rowSums(eo_dtm) < 10)
which(rowSums(eo_dtm)== 2)
head(rownames(eo_dtm))
rownames(eo_dtm)[10002]
eos[eos$num == 10002]
eos$text[eos$num == 10002]
eos$text[ eos$word.count < 10]
eos$text[ eos$word.count < 20]
eos$text[ eos$word.count < 50]
eos$text[ eos$word.count < 10]
eos$text[ eos$word.count < 20]
rm(list = ls())
# Load packages, functions, and data
source("scripts/util/__Util_MASTER.R")
load("data/eosRaw.new.RData")
#remove a few FDR EOs that were scraped from the 1945 page
eos <- filter(eos, !(num %in% c("9511", "9523",
"9524", "9531",
"9533")))
#remove unnecessary html snippets
eos$html <- str_replace_all(string      = eos$html,
pattern     = "</div></div><span class=\"displaytext\">",
replacement = "")
eos$html <- str_replace_all(string      = eos$html,
pattern     = "</span><hr noshade=\"noshade\" size=\"1\"><span class=\"displaynotes\"><i></i></span><hr noshade=\"noshade\" size=\"1\">",
replacement = "")
#remove non-breaking spaces
eos$html <- str_replace_all(string      = eos$html,
pattern     = "&nbsp;",
replacement = " ")
#replace unicode apostrophes
eos$html <- str_replace_all(string      = eos$html,
pattern     = "\u0092",
replacement = "'")
#replace other unicode characters
eos$html <- iconv(x    = eos$html,
from = "utf-8",
to   = "ascii",
sub  = " ")
#remove html tags
eos$text <- str_replace_all(string      = eos$html,
pattern     = "<.*?>",
replacement = " ")
##############################
# Remove Unpublished EOs
##############################
#convert multiple spaces to single spaces
eos$text <- str_replace_all(string      = eos$text,
pattern     = "\\s+",
replacement = " ")
#rough word count
eos$word.count <- str_count(string  = eos$text,
pattern = " ") + 1
summary(eos$word.count)
shortEOs <- filter(eos, eos$word.count < 10)
unpublishedEOs <- shortEOs$num
shortEOs$text
# Find and remove unpublished EOs
shortEOs <- filter(eos, eos$word.count < 10)
unpublishedEOs <- shortEOs$num
eos <- filter(eos, word.count > 10)
summary(eos$word.count)
source('~/Documents/Research/Executive Orders/executive_orders/scripts/01-curateEOs.R', echo=TRUE)
source('~/Documents/Research/Executive Orders/executive_orders/scripts/01-curateEOs.R', echo=TRUE)
source('~/Documents/Research/Executive Orders/executive_orders/scripts/01-curateEOs.R', echo=TRUE)
pres
pres[1]
pres[2]
pres <- do.call('rbind', pres)
#remove letters to make EO matching simpler
eos$num2 <- str_replace_all(string      = eos$num,
pattern     = "-?[A-Z]$",
replacement = "")
eos <- merge(x     = eos,
y     = pres,
by    = "num2",
all.x = TRUE)
eos <- select(eos, -num2)
missingEOs <- merge(x     = missingEOs,
y     = pres,
by    = "num2",
all.x = TRUE)
#remove Trump EOs
eos <- eos[eos$president != "45-Trump", ]
source('~/Documents/Research/Executive Orders/executive_orders/scripts/01-curateEOs.R', echo=TRUE)
names(eos)
test <- eos %>% group_by(president) %>% summarise(freq = n())
test
plot(test)
plot(test$freq)
eos$num[grepl(eos$president, "obama")]
eos$num[grepl(eos$president, "Obama")]
eos$num[grepl("Obama", eos$president)]
rm(list =ls())
source("scripts/util/__Util_MASTER.R")
library(parallel)
library(snowfall)
####################
# Load data
####################
load("/scratch/gpfs/ctokita/ExecutiveOrders/data/eosCurated.RData")
load("data_derived/dtms/eo_dtm.Rdata")
# Remove overly frequent words
tf <- TermDocFreq(dtm = eo_dtm)
keep_terms <- tf$term[ tf$doc_freq <= nrow(eo_dtm)/2]
eo_dtm <- eo_dtm[, keep_terms]
summary(rowSums(eo_dtm)) #check to make sure all documents have words
View(tf)
test <- ("science", "scientific", "environment", "environmental")
test <- c("science", "scientific", "environment", "environmental")
library(SnowballC)
install.packages("corpus")
library(corpus)
text_tokens(test)
text_token(c("love", "loved"))
text_tokens(c("love", "loved"))
text_tokens(c("love", "loved"), stemmer = "en")
text_tokens(test, stemmer = "en")
hunspell::hunspell_stem("science")
hunspell::hunspell_stem("sciences")
hunspell::hunspell_stem("scienctific")
hunspell::hunspell_stem("environmental")
?CreateDtm
stopwords::stopwords(source = "en")
stopwords::stopwords(source = "smart")
source("scripts/util/__Util_MASTER.R")
####################
# Load data
####################
files <_ list.files("data_derived/lda_models/", full.names = T)
####################
# Load data
####################
files <- list.files("data_derived/lda_models/", full.names = T)
x = files[1]
# Load
load(x)
names(eo_lda)
eo_lda$coherence
# Get coherence data
to_return(topic_num = length(eo_lda$coherence),
coherence = eo_lda$coherence)
to_return <- data.frame(topic_num = length(eo_lda$coherence),
coherence = eo_lda$coherence)
View(to_return)
source("scripts/util/__Util_MASTER.R")
####################
# Analyze coherence by topic number
####################
# Get files
files <- list.files("data_derived/lda_models/", full.names = T)
# load each and get coherence
coherence <- lapply(files, function(x) {
# Load
load(x)
# Get coherence data
to_return <- data.frame(topic_num = length(eo_lda$coherence),
coherence = eo_lda$coherence)
# Return
return(to_return)
})
# load each and get coherence
coherence <- lapply(files, function(x) {
# Load
print(x)
load(x)
# Get coherence data
to_return <- data.frame(topic_num = length(eo_lda$coherence),
coherence = eo_lda$coherence)
# Return
return(to_return)
})
coherence <- do.call('rbind', coherence)
View(coherence)
plot(coherence)
rm(to_return)
# Prep and plot
cohere_data <- coherence %>%
group_by(topc_num) %>%
mutate(mean_cohere = mean(coherence))
# Prep and plot
cohere_data <- coherence %>%
group_by(topic_num) %>%
mutate(mean_cohere = mean(coherence))
View(cohere_data)
# Prep and plot
cohere_data <- coherence %>%
group_by(topic_num) %>%
summarise(mean_cohere = mean(coherence),
med_cohere = median(coherence))
gg_cohere <- ggplot() +
geom_point(data = coherence, aes(x = topic_num, y = coherence))
source("scripts/util/__Util_MASTER.R")
gg_cohere <- ggplot() +
geom_point(data = coherence, aes(x = topic_num, y = coherence))
####################
# Source other function scripts
####################
source("scripts/util/__Util_GraphingFunctions.R")
gg_cohere <- ggplot() +
geom_point(data = coherence, aes(x = topic_num, y = coherence), size = 0.1, color = "grey20") +
geom_line(data = cohere_data, aes(x = topic_num, y = mean_cohere)) +
geom_point(data = cohere_data, aes(x = topic_num, y = mean_cohere)) +
theme_ctokita()
gg_cohere
gg_cohere <- ggplot() +
geom_point(data = coherence, aes(x = topic_num, y = coherence), size = 0.1, color = "grey20") +
geom_line(data = cohere_data, aes(x = topic_num, y = med_cohere)) +
geom_point(data = cohere_data, aes(x = topic_num, y = med_cohere)) +
theme_ctokita()
gg_cohere
gg_cohere <- ggplot() +
# geom_point(data = coherence, aes(x = topic_num, y = coherence), size = 0.1, color = "grey20") +
geom_line(data = cohere_data, aes(x = topic_num, y = mean_cohere)) +
geom_point(data = cohere_data, aes(x = topic_num, y = mean_cohere)) +
theme_ctokita()
gg_cohere
gg_cohere <- ggplot() +
geom_point(data = coherence, aes(x = topic_num, y = coherence), size = 0.1, color = "grey80") +
geom_line(data = cohere_data, aes(x = topic_num, y = mean_cohere)) +
geom_point(data = cohere_data, aes(x = topic_num, y = mean_cohere)) +
theme_ctokita()
gg_cohere
# Prep and plot
cohere_data <- coherence %>%
mutate(coherence = log10(coherence))
cohere_sum <- coherence_data %>%
group_by(topic_num) %>%
summarise(mean_cohere = mean(coherence),
med_cohere = median(coherence))
cohere_sum <- cohere_data %>%
group_by(topic_num) %>%
summarise(mean_cohere = mean(coherence),
med_cohere = median(coherence))
gg_cohere <- ggplot() +
geom_point(data = coherence, aes(x = topic_num, y = coherence), size = 0.1, color = "grey80") +
geom_line(data = cohere_data, aes(x = topic_num, y = mean_cohere)) +
geom_point(data = cohere_data, aes(x = topic_num, y = mean_cohere)) +
theme_ctokita()
gg_cohere
gg_cohere <- ggplot() +
geom_point(data = cohere_data, aes(x = topic_num, y = coherence), size = 0.1, color = "grey80") +
geom_line(data = cohere_sum, aes(x = topic_num, y = mean_cohere)) +
geom_point(data = cohere_sum, aes(x = topic_num, y = mean_cohere)) +
theme_ctokita()
gg_cohere
# Prep and plot
cohere_data <- coherence %>%
mutate(coherence = log(coherence))
cohere_sum <- cohere_data %>%
group_by(topic_num) %>%
summarise(mean_cohere = mean(coherence),
med_cohere = median(coherence))
gg_cohere <- ggplot() +
geom_point(data = cohere_data, aes(x = topic_num, y = coherence), size = 0.1, color = "grey80") +
geom_line(data = cohere_sum, aes(x = topic_num, y = mean_cohere)) +
geom_point(data = cohere_sum, aes(x = topic_num, y = mean_cohere)) +
theme_ctokita()
gg_cohere
# Prep and plot
cohere_data <- coherence
cohere_sum <- cohere_data %>%
group_by(topic_num) %>%
summarise(mean_cohere = mean(coherence),
med_cohere = median(coherence))
gg_cohere <- ggplot() +
geom_point(data = cohere_data, aes(x = topic_num, y = coherence), size = 0.1, color = "grey80") +
geom_line(data = cohere_sum, aes(x = topic_num, y = mean_cohere)) +
geom_point(data = cohere_sum, aes(x = topic_num, y = mean_cohere)) +
theme_ctokita()
gg_cohere
gg_cohere <- ggplot() +
# geom_point(data = cohere_data, aes(x = topic_num, y = coherence), size = 0.1, color = "grey80") +
geom_line(data = cohere_sum, aes(x = topic_num, y = mean_cohere)) +
geom_point(data = cohere_sum, aes(x = topic_num, y = mean_cohere)) +
theme_ctokita()
gg_cohere
gg_cohere <- ggplot() +
# geom_point(data = cohere_data, aes(x = topic_num, y = coherence), size = 0.1, color = "grey80") +
geom_line(data = cohere_sum, aes(x = topic_num, y = med_cohere)) +
geom_point(data = cohere_sum, aes(x = topic_num, y = med_cohere)) +
theme_ctokita()
gg_cohere
load('data_derived/dtms/eo_dtm.Rdata')
# Remove overly frequent words
tf <- TermDocFreq(dtm = eo_dtm)
keep_terms <- tf$term[ tf$doc_freq <= nrow(eo_dtm)/2]
rnow(tf) - nrow(keep_terms)
length(keep_terms)
rnow(tf) - length(keep_terms)
nrow(tf) - length(keep_terms)
eo_dtm
colSums(eo_dtm) / sum(eo_dtm)
colSums(eo_dtm) / sum(eo_dtm) * 100
50/50
50/10
50/10
plot(eo_lda$log_likelihood, type = "l")
gg_cohere
rm(list = ls())
# Load bet fit model
load("data_derived/lda_models/eo_lda_k60.Rdata")
plot(eo_lda$log_likelihood, type = "l")
# Get top terms and label topics
eo_lda$top_terms <- GetTopTerms(phi = eo_lda$phi, M = 10)
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 5,
dtm = eo_dtm,
M = 3)
# Summarise
eo_lda$prevalence <- colSums(eo_lda$theta) / sum(eo_lda$theta) * 100
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 0.05,
dtm = eo_dtm,
M = 3)
# Load bet fit model
load("data_derived/dtms/eo_dtm.Rdata")
load("data_derived/lda_models/eo_lda_k60.Rdata")
plot(eo_lda$log_likelihood, type = "l")
# Get top terms and label topics
eo_lda$top_terms <- GetTopTerms(phi = eo_lda$phi, M = 10)
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 0.05,
dtm = eo_dtm,
M = 3)
# Summarise
eo_lda$prevalence <- colSums(eo_lda$theta) / sum(eo_lda$theta) * 100
eo_lda$summary <- data.frame(topic = rownames(eo_lda$phi),
label = eo_lda$labels,
coherence = round(eo_lda$coherence, 3),
prevalence = round(eo_lda$prevalence,3),
top_terms = apply(eo_lda$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
test <- eo_lda$summary
View(test)
rm(list = ls())
# Load bet fit model
load("data_derived/dtms/eo_dtm.Rdata")
load("data_derived/lda_models/eo_lda_k60.Rdata")
plot(eo_lda$log_likelihood, type = "l")
# Get top terms and label topics
eo_lda$top_terms <- GetTopTerms(phi = eo_lda$phi, M = 10)
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 0.05,
dtm = eo_dtm,
M = 3)
# Summarise
eo_lda$prevalence <- colSums(eo_lda$theta) / sum(eo_lda$theta) * 100
eo_lda$summary <- data.frame(topic = rownames(eo_lda$phi),
label = eo_lda$labels,
coherence = round(eo_lda$coherence, 3),
prevalence = round(eo_lda$prevalence,3),
top_terms = apply(eo_lda$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
test <- eo_lda$summary
View(test)
eo_lda$top_terms
grepl("scien", eo_lda$top_terms)
sum(grepl("scien", eo_lda$top_terms))
rm(list = ls())
# Load bet fit model
load("data_derived/dtms/eo_dtm.Rdata")
load("data_derived/lda_models/eo_lda_k85.Rdata")
plot(eo_lda$log_likelihood, type = "l")
# Get top terms and label topics
eo_lda$top_terms <- GetTopTerms(phi = eo_lda$phi, M = 10)
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 0.05,
dtm = eo_dtm,
M = 3)
# Summarise
eo_lda$prevalence <- colSums(eo_lda$theta) / sum(eo_lda$theta) * 100
eo_lda$summary <- data.frame(topic = rownames(eo_lda$phi),
label = eo_lda$labels,
coherence = round(eo_lda$coherence, 3),
prevalence = round(eo_lda$prevalence,3),
top_terms = apply(eo_lda$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
sum(grepl("scien", eo_lda$top_terms))
test <- eo_lda$summary
View(test)
sum(grepl("tech", eo_lda$top_terms))
sum(grepl("research", eo_lda$top_terms))
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 0.2,
dtm = eo_dtm,
M = 3)
eo_lda$summary <- data.frame(topic = rownames(eo_lda$phi),
label = eo_lda$labels,
coherence = round(eo_lda$coherence, 3),
prevalence = round(eo_lda$prevalence,3),
top_terms = apply(eo_lda$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
test <- eo_lda$summary
View(test)
View(eo_lda)
View(test)
?LavelTopics
?LabelTopics
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 0.4,
dtm = eo_dtm,
M = 3)
# Summarise
eo_lda$prevalence <- colSums(eo_lda$theta) / sum(eo_lda$theta) * 100
eo_lda$summary <- data.frame(topic = rownames(eo_lda$phi),
label = eo_lda$labels,
coherence = round(eo_lda$coherence, 3),
prevalence = round(eo_lda$prevalence,3),
top_terms = apply(eo_lda$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
test <- eo_lda$summary
View(test)
eo_lda$top_terms <- GetTopTerms(phi = eo_lda$phi, M = 10)
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 0.1,
dtm = eo_dtm,
M = 3)
# Summarise
eo_lda$prevalence <- colSums(eo_lda$theta) / sum(eo_lda$theta) * 100
eo_lda$summary <- data.frame(topic = rownames(eo_lda$phi),
label = eo_lda$labels,
coherence = round(eo_lda$coherence, 3),
prevalence = round(eo_lda$prevalence,3),
top_terms = apply(eo_lda$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
test <- eo_lda$summary
View(test)
eo_lda$phi
eo_lda$log_likelihood
test <- eo_lda[, "log_likelihood"]
test <- eo_lda["log_likelihood"]
