####################
# Get stopwords
stop_words <- paste(c("\\b", stopwords('en'), "\\b"), collapse = "\\b|\\b")
stop_words
####################
# Prep data
####################
# Get stopwords
stop_words <- paste(stopwords('en'), collapse = "\\b|\\b")
stop_words <- paste0('\\b', stop_words, '\\b')
####################
# Prep data
####################
# Get stopwords
stop_words <- paste(stopwords('en'), collapse = "\\b|\\b")
stop_words <- paste0('\\b', stop_words, '\\b')
# Process
eo_text <- eos %>%
select(num, text) %>%
mutate(text = gsub("[^A-Za-z ]", "", text)) %>% # remove non-letters
mutate(text = tolower(text)) %>% # make lowercase
mutate(text = gsub(stop_words, "", text)) %>% # remove stopwords
mutate(text = gsub("^[ ]+", "", text)) %>% # remove leading whitespace
mutate(text = gsub("[ ]+$", "", text)) %>% # remove trailing whitespace
mutate(text = gsub("[ ]+", " ", text))  # remove extra space
eo_text$text[1]
eos$text[1]
eos$text[1]
# Process
eo_text <- eos %>%
select(num, text) %>%
mutate(text = gsub("\\b[0-9]+st\\b|\\b[0-9]+nd\\b|\\b[0-9]+th\\b", "", text)) %>% # remove 1st, 2nd, etc.
mutate(text = gsub("\\([a-z]\\)", "", text)) %>% # remove (a), (b), etc.
mutate(text = gsub("[^A-Za-z ]", "", text)) %>% # remove non-letters
mutate(text = tolower(text)) %>% # make lowercase
mutate(text = gsub(stop_words, "", text)) %>% # remove stopwords
mutate(text = gsub("^[ ]+", "", text)) %>% # remove leading whitespace
mutate(text = gsub("[ ]+$", "", text)) %>% # remove trailing whitespace
mutate(text = gsub("[ ]+", " ", text))  # remove extra space
eo_text$text[1]
# Process
eo_text <- eos %>%
select(num, text) %>%
mutate(text = gsub("\\b[0-9]+st\\b|\\b[0-9]+nd\\b|\\b[0-9]+th\\b", "", text)) %>% # remove 1st, 2nd, etc.
mutate(text = gsub("\\([a-z]\\)", "", text)) %>% # remove (a), (b), etc.
mutate(text = gsub("[^A-Za-z ]", "", text)) %>% # remove non-letters
mutate(text = tolower(text)) %>% # make lowercase
mutate(text = gsub(stop_words, "", text)) %>% # remove stopwords
mutate(text = gsub("^[ ]+", "", text)) %>% # remove leading whitespace
mutate(text = gsub("[ ]+$", "", text)) %>% # remove trailing whitespace
mutate(text = gsub("[ ]+", " ", text)) %>%  # remove extra space
mutate(text = stemDocument(text))
eo_text$text[1]
# Make document-term matrix
eo_dtm <- DocumentTermMatrix(eo_text)
# Make document-term matrix
eo_dtm <- DocumentTermMatrix(eo_text$text)
# Make document-term matrix
eo_words <- eo_text %>%
unnest_tokens(word, text)
source("scripts/util/__Util_MASTER.R")
# Make document-term matrix
eo_words <- eo_text %>%
unnest_tokens(word, text)
View(eo_words)
names(eo_words)
# Count words by document
word_counts <- eo_words %>%
count(num, word, sort = TRUE)
View(word_counts)
# Count words by document
document_dtm <- word_counts %>%
cast_dtm(num, word, n)
View(document_dtm)
document_dtm
# Count words by document
eo_dtm <- word_counts %>%
cast_dtm(num, word, n)
####################
# Topic model
####################
eo_lda <- LDA(eo_dtm, k = 20, control = list(seed = 323))
####################
# Interpret model
####################
eo_topics <- tiedy(eo_lda, matrix = "beta")
####################
# Interpret model
####################
eo_topics <- tidy(eo_lda, matrix = "beta")
View(eo_topics)
eo_top_terms <- eo_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
View(eo_top_terms)
source("scripts/000-utilityFunctionsEO.R")
source("scripts/util/__Util_MASTER.R")
source("scripts/util/__Util_MASTER.R")
stopwards("en")
stopwords('en')
stopwords('smart')
source("scripts/util/__Util_MASTER.R")
####################
# Load data
####################
load("data/eosCurated.RData")
stopwords('en')
source("scripts/util/__Util_MASTER.R")
names(eos)
####################
# Make document-term matrix (DTM)
####################
# create DTM
eo_detm <- CreateDtm(doc_vec = eos$text,
doc_names = eos$num,
ngram_window = c(1, 2),
stopword_vec = c(stopwords('en'), stopwords('smart')),
lower = TRUE,
remove_punctuation = TRUE,
remove_numbers = TRUE,
verbose = FALSE,
cpus = 2)
eo_dtm <- eo_detm
rm(eo_detm)
dim(eo_dtm)
head(colnames(eo_dtm))
?CreateDTM
?CreateDtm
?stemDocument
####################
# Make document-term matrix (DTM)
####################
# create DTM
eo_dtm <- CreateDtm(doc_vec = eos$text,
doc_names = eos$num,
ngram_window = c(1, 2),
stopword_vec = c(stopwords('en'), stopwords('smart')),
lower = TRUE,
remove_punctuation = TRUE,
remove_numbers = TRUE,
verbose = FALSE,
stem_lemma_function = stemDocument(),
cpus = 2)
####################
# Make document-term matrix (DTM)
####################
# create DTM
eo_dtm <- CreateDtm(doc_vec = eos$text,
doc_names = eos$num,
ngram_window = c(1, 2),
stopword_vec = c(stopwords('en'), stopwords('smart')),
lower = TRUE,
remove_punctuation = TRUE,
remove_numbers = TRUE,
verbose = FALSE,
stem_lemma_function = stemDocument(x),
cpus = 2)
####################
# Make document-term matrix (DTM)
####################
# create DTM
eo_dtm <- CreateDtm(doc_vec = eos$text,
doc_names = eos$num,
ngram_window = c(1, 2),
stopword_vec = c(stopwords('en'), stopwords('smart')),
lower = TRUE,
remove_punctuation = TRUE,
remove_numbers = TRUE,
verbose = FALSE,
stem_lemma_function = function(x) SnowballC::wordStem(x, "porter"),
cpus = 2)
head(colnames(eo_dtm))
# Basic corpus statistics
tf_mat <- TermDocFreq(eo_dtm)
head(tf_mat[order(tf_mat$term_freq, decreasing = TRUE)])
head(tf_mat[order(tf_mat$term_freq, decreasing = TRUE), ])
head(tf_mat[order(tf_mat$term_freq, decreasing = TRUE), ], 20)
####################
# Prep data
####################
# make document-term matrix (DTM)
eo_dtm <- CreateDtm(doc_vec = eos$text,
doc_names = eos$num,
ngram_window = c(1, 2),
stopword_vec = c(stopwords('en'), stopwords('smart')),
lower = TRUE,
remove_punctuation = TRUE,
remove_numbers = TRUE,
verbose = FALSE,
# stem_lemma_function = function(x) SnowballC::wordStem(x, "porter"),
cpus = 2)
# Basic corpus statistics
tf_mat <- TermDocFreq(eo_dtm)
head(tf_mat[order(tf_mat$term_freq, decreasing = TRUE), ], 20) # see top words
tf_bigrams <- tf_mat[ stringr::str_detect(tf_mat$term, "_") , ]
head(tf_bigrams[ order(tf_bigrams$term_freq, decreasing = TRUE) , ], 20)
####################
# Prep data
####################
# make document-term matrix (DTM)
eo_dtm <- CreateDtm(doc_vec = eos$text,
doc_names = eos$num,
ngram_window = c(1, 2),
stopword_vec = c(stopwords('en'), stopwords('smart')),
lower = TRUE,
remove_punctuation = TRUE,
remove_numbers = TRUE,
verbose = FALSE,
stem_lemma_function = function(x) SnowballC::wordStem(x, "porter"),
cpus = 2)
# Basic corpus statistics
tf_mat <- TermDocFreq(eo_dtm)
head(tf_mat[order(tf_mat$term_freq, decreasing = TRUE), ], 20) # see top words
tf_bigrams <- tf_mat[ stringr::str_detect(tf_mat$term, "_") , ]
head(tf_bigrams[ order(tf_bigrams$term_freq, decreasing = TRUE) , ], 20)
eo_dtm <- CreateDtm(doc_vec = eos$text,
doc_names = eos$num,
ngram_window = c(1, 2),
stopword_vec = c(stopwords('en'), stopwords('smart')),
lower = TRUE,
remove_punctuation = TRUE,
remove_numbers = TRUE,
verbose = FALSE,
# stem_lemma_function = function(x) SnowballC::wordStem(x, "porter"),
cpus = 2)
# Basic corpus statistics
tf_mat <- TermDocFreq(eo_dtm)
head(tf_mat[order(tf_mat$term_freq, decreasing = TRUE), ], 20) # see top words
tf_bigrams <- tf_mat[ stringr::str_detect(tf_mat$term, "_") , ]
head(tf_bigrams[ order(tf_bigrams$term_freq, decreasing = TRUE) , ], 20) # see top bigram words
head(tf_mat)
exp(1)
exp(8.35)
exp(0.8)
####################
# Document clustering
####################
# Cosine similarity using inverse document frequency (IDF)
tfdif <- t(dtm[, tf_mat$term]) * tf_mat$idf
####################
# Document clustering
####################
# Cosine similarity using inverse document frequency (IDF)
tfdif <- t(eo_dtm[, tf_mat$term]) * tf_mat$idf
tfdif <- t(tfdif)
head(tfdif)
csim <- tfdif / sqrt(rowSums(tfidf * tfidf))
rm(tfdif)
####################
# Document clustering
####################
# Cosine similarity using inverse document frequency (IDF)
tfidf <- t(eo_dtm[, tf_mat$term]) * tf_mat$idf # reweight terms by IDF
tfidf <- t(tfidf)
csim <- tfidf / sqrt(rowSums(tfidf * tfidf))
head(csim)
csim <- csim %*% t(csim)
csim
cdist <- as.dist(1 - csim) #make it distance
cdist
head(cdist)
hc <- hclust(cdist, "ward.D")
clustering <- cutree(hc, 10)
plot(hc, main = "Hierarchical clustering of 100 NIH grant abstracts",
ylab = "", xlab = "", yaxt = "n")
rect.hclust(hc, 10, border = "red")
clustering <- cutree(hc, 20)
plot(hc, main = "Hierarchical clustering of 100 NIH grant abstracts",
ylab = "", xlab = "", yaxt = "n")
rect.hclust(hc, 20, border = "red")
# Cluster summary
p_words <- colSums(dtm) / sum(dtm)
# Cluster summary
p_words <- colSums(eo_dtm) / sum(eo_dtm)
cluster_words <- lapply(unique(clustering), function(x){
rows <- eo_dtm[ clustering == x , ]
# for memory's sake, drop all words that don't appear in the cluster
rows <- rows[ , colSums(rows) > 0 ]
colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})
cluster_summary <- data.frame(cluster = unique(clustering),
size = as.numeric(table(clustering)),
top_words = sapply(cluster_words, function(d){
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:5 ],
collapse = ", ")
}),
stringsAsFactors = FALSE)
View(cluster_summary)
tfidf <- t(eo_dtm[, tf_mat$term]) * tf_mat$idf # reweight terms by IDF
tfidf <- t(tfidf)
csim <- tfidf / sqrt(rowSums(tfidf * tfidf)) # calculate cosine similiarity
csim <- csim %*% t(csim)
cdist <- as.dist(1 - csim) #make it distance
hc <- hclust(cdist, "ward.D")
clustering <- cutree(hc, 80)
plot(hc, main = "Hierarchical clustering of Executive Orders",
ylab = "", xlab = "", yaxt = "n")
rect.hclust(hc, 80, border = "red")
# Cluster summary
p_words <- colSums(eo_dtm) / sum(eo_dtm)
cluster_words <- lapply(unique(clustering), function(x){
rows <- eo_dtm[ clustering == x , ]
# for memory's sake, drop all words that don't appear in the cluster
rows <- rows[ , colSums(rows) > 0 ]
colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})
cluster_summary <- data.frame(cluster = unique(clustering),
size = as.numeric(table(clustering)),
top_words = sapply(cluster_words, function(d){
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:5 ],
collapse = ", ")
}),
stringsAsFactors = FALSE)
View(cluster_summary)
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:20 ],
collapse = ", ")
cluster_summary <- data.frame(cluster = unique(clustering),
size = as.numeric(table(clustering)),
top_words = sapply(cluster_words, function(d){
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:20 ],
collapse = ", ")
}),
stringsAsFactors = FALSE)
View(cluster_summary)
tfidf <- t(eo_dtm[, tf_mat$term]) * tf_mat$idf # reweight terms by IDF
tfidf <- t(tfidf)
csim <- tfidf / sqrt(rowSums(tfidf * tfidf)) # calculate cosine similiarity
csim <- csim %*% t(csim)
cdist <- as.dist(1 - csim) #make it distance
hc <- hclust(cdist, "ward.D")
clustering <- cutree(hc, 150)
plot(hc, main = "Hierarchical clustering of Executive Orders",
ylab = "", xlab = "", yaxt = "n")
rect.hclust(hc, 150, border = "red")
# Cluster summary
p_words <- colSums(eo_dtm) / sum(eo_dtm)
cluster_words <- lapply(unique(clustering), function(x){
rows <- eo_dtm[ clustering == x , ]
# for memory's sake, drop all words that don't appear in the cluster
rows <- rows[ , colSums(rows) > 0 ]
colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})
cluster_summary <- data.frame(cluster = unique(clustering),
size = as.numeric(table(clustering)),
top_words = sapply(cluster_words, function(d){
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:10 ],
collapse = ", ")
}),
stringsAsFactors = FALSE)
View(cluster_summary)
####################
# Topic model
####################
set.seed(323)
eo_lda <- FitLdaModel(dtm = eo_dtm,
k = 80,
iterations = 1000,
burnin = 180,
alpha = 0.1,
beta = 0.05,
optimize_alpha = TRUE,
calc_likelihood = TRUE,
calc_coherence = TRUE,
calc_r2 = TRUE,
cpus = 2)
####################
# Interpret model
####################
plot(model$log_likelihood, type = "l")
####################
# Interpret model
####################
plot(eo_lda$log_likelihood, type = "l")
eo_lda$r2
hist(eo_lda$coherence)
head(t(eo$top_terms))
head(t(eo_lda$top_terms))
head(t(eo_lda$top_terms))
model$top_terms <- GetTopTerms(phi = model$phi, M = 10)
eo_lda$top_terms <- GetTopTerms(phi = eo_lda$phi, M = 10)
head(t(eo_lda$top_terms))
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 0.05,
dtm = eo_dtm,
M = 1)
eo_lda$labels
eo_lda$summary <- data.frame(topic = rownames(eo_lda$phi),
label = eo_lda$labels,
coherence = round(eo_lda$coherence, 3),
prevalence = round(eo_lda$prevalence,3),
top_terms = apply(eo_lda$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
# Summarise
eo_lda$prevalence <- colSums(eo_lda$theta) / sum(eo_lda$theta) * 100
eo_lda$summary <- data.frame(topic = rownames(eo_lda$phi),
label = eo_lda$labels,
coherence = round(eo_lda$coherence, 3),
prevalence = round(eo_lda$prevalence,3),
top_terms = apply(eo_lda$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
View(eo_lda$summary)
?FitLdaModel
####################
# Interpret model
####################
plot(eo_lda$log_likelihood, type = "l")
abline(v = 180)
mean(eo_lda$coherence)
mean(eo_lda$coherence * eo_lda$prevalence)
eo_lda$coherence * eo_lda$prevalence
eo_lda$prevalence
eo_lda$coherence
View(cluster_summary)
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 0.1,
dtm = eo_dtm,
M = 1)
eo_lda$summary <- data.frame(topic = rownames(eo_lda$phi),
label = eo_lda$labels,
coherence = round(eo_lda$coherence, 3),
prevalence = round(eo_lda$prevalence,3),
top_terms = apply(eo_lda$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
View(cluster_summary)
eo_lda$labels
View(eo_lda$summary)
eo_lda$theta
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 0.1,
dtm = eo_dtm,
M = 3)
eo_lda$summary <- data.frame(topic = rownames(eo_lda$phi),
label = eo_lda$labels,
coherence = round(eo_lda$coherence, 3),
prevalence = round(eo_lda$prevalence,3),
top_terms = apply(eo_lda$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
View(eo_lda$summary)
test <- eo_lda$summary
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 0.2,
dtm = eo_dtm,
M = 3)
# Summarise
eo_lda$prevalence <- colSums(eo_lda$theta) / sum(eo_lda$theta) * 100
eo_lda$summary <- data.frame(topic = rownames(eo_lda$phi),
label = eo_lda$labels,
coherence = round(eo_lda$coherence, 3),
prevalence = round(eo_lda$prevalence,3),
top_terms = apply(eo_lda$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
View(eo_lda$summary)
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 0,
dtm = eo_dtm,
M = 3)
# Save DTM
save(eo_dtm, "data_derived/eo_dtm.Rdata")
# Save DTM
save(eo_dtm, file = "data_derived/eo_dtm.Rdata")
source("scripts/util/__Util_MASTER.R")
library(parallel)
library(snowfall)
####################
# Load data
####################
load("data/eosCurated.RData")
##################################################################################################
#
# Topic model EOs
#
##################################################################################################
rm(list =ls())
rm(list =ls())
source("scripts/util/__Util_MASTER.R")
library(parallel)
library(snowfall)
####################
# Load data
####################
load("data/eosCurated.RData")
####################
# Topic model
####################
# Set topic numbers to test
Ks <- seq(10, 100, 5)
k = 85
# Save
file_name <- paste0("EO_lda_k", k)
file_name
# Save
file_name <- paste0("eo_lda_k", k)
file_name
Ks
####################
# Topic model
####################
# Set topic numbers to test
Ks <- seq(20, 110, 5)
Ks
