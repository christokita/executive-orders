ngram_window = c(1, 2),
stopword_vec = c(stopwords('en'), stopwords('smart')),
lower = TRUE,
remove_punctuation = TRUE,
remove_numbers = TRUE,
verbose = FALSE,
stem_lemma_function = function(x) SnowballC::wordStem(x, "porter"),
cpus = 2)
head(colnames(eo_dtm))
# Basic corpus statistics
tf_mat <- TermDocFreq(eo_dtm)
head(tf_mat[order(tf_mat$term_freq, decreasing = TRUE)])
head(tf_mat[order(tf_mat$term_freq, decreasing = TRUE), ])
head(tf_mat[order(tf_mat$term_freq, decreasing = TRUE), ], 20)
####################
# Prep data
####################
# make document-term matrix (DTM)
eo_dtm <- CreateDtm(doc_vec = eos$text,
doc_names = eos$num,
ngram_window = c(1, 2),
stopword_vec = c(stopwords('en'), stopwords('smart')),
lower = TRUE,
remove_punctuation = TRUE,
remove_numbers = TRUE,
verbose = FALSE,
# stem_lemma_function = function(x) SnowballC::wordStem(x, "porter"),
cpus = 2)
# Basic corpus statistics
tf_mat <- TermDocFreq(eo_dtm)
head(tf_mat[order(tf_mat$term_freq, decreasing = TRUE), ], 20) # see top words
tf_bigrams <- tf_mat[ stringr::str_detect(tf_mat$term, "_") , ]
head(tf_bigrams[ order(tf_bigrams$term_freq, decreasing = TRUE) , ], 20)
####################
# Prep data
####################
# make document-term matrix (DTM)
eo_dtm <- CreateDtm(doc_vec = eos$text,
doc_names = eos$num,
ngram_window = c(1, 2),
stopword_vec = c(stopwords('en'), stopwords('smart')),
lower = TRUE,
remove_punctuation = TRUE,
remove_numbers = TRUE,
verbose = FALSE,
stem_lemma_function = function(x) SnowballC::wordStem(x, "porter"),
cpus = 2)
# Basic corpus statistics
tf_mat <- TermDocFreq(eo_dtm)
head(tf_mat[order(tf_mat$term_freq, decreasing = TRUE), ], 20) # see top words
tf_bigrams <- tf_mat[ stringr::str_detect(tf_mat$term, "_") , ]
head(tf_bigrams[ order(tf_bigrams$term_freq, decreasing = TRUE) , ], 20)
eo_dtm <- CreateDtm(doc_vec = eos$text,
doc_names = eos$num,
ngram_window = c(1, 2),
stopword_vec = c(stopwords('en'), stopwords('smart')),
lower = TRUE,
remove_punctuation = TRUE,
remove_numbers = TRUE,
verbose = FALSE,
# stem_lemma_function = function(x) SnowballC::wordStem(x, "porter"),
cpus = 2)
# Basic corpus statistics
tf_mat <- TermDocFreq(eo_dtm)
head(tf_mat[order(tf_mat$term_freq, decreasing = TRUE), ], 20) # see top words
tf_bigrams <- tf_mat[ stringr::str_detect(tf_mat$term, "_") , ]
head(tf_bigrams[ order(tf_bigrams$term_freq, decreasing = TRUE) , ], 20) # see top bigram words
head(tf_mat)
exp(1)
exp(8.35)
exp(0.8)
####################
# Document clustering
####################
# Cosine similarity using inverse document frequency (IDF)
tfdif <- t(dtm[, tf_mat$term]) * tf_mat$idf
####################
# Document clustering
####################
# Cosine similarity using inverse document frequency (IDF)
tfdif <- t(eo_dtm[, tf_mat$term]) * tf_mat$idf
tfdif <- t(tfdif)
head(tfdif)
csim <- tfdif / sqrt(rowSums(tfidf * tfidf))
rm(tfdif)
####################
# Document clustering
####################
# Cosine similarity using inverse document frequency (IDF)
tfidf <- t(eo_dtm[, tf_mat$term]) * tf_mat$idf # reweight terms by IDF
tfidf <- t(tfidf)
csim <- tfidf / sqrt(rowSums(tfidf * tfidf))
head(csim)
csim <- csim %*% t(csim)
csim
cdist <- as.dist(1 - csim) #make it distance
cdist
head(cdist)
hc <- hclust(cdist, "ward.D")
clustering <- cutree(hc, 10)
plot(hc, main = "Hierarchical clustering of 100 NIH grant abstracts",
ylab = "", xlab = "", yaxt = "n")
rect.hclust(hc, 10, border = "red")
clustering <- cutree(hc, 20)
plot(hc, main = "Hierarchical clustering of 100 NIH grant abstracts",
ylab = "", xlab = "", yaxt = "n")
rect.hclust(hc, 20, border = "red")
# Cluster summary
p_words <- colSums(dtm) / sum(dtm)
# Cluster summary
p_words <- colSums(eo_dtm) / sum(eo_dtm)
cluster_words <- lapply(unique(clustering), function(x){
rows <- eo_dtm[ clustering == x , ]
# for memory's sake, drop all words that don't appear in the cluster
rows <- rows[ , colSums(rows) > 0 ]
colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})
cluster_summary <- data.frame(cluster = unique(clustering),
size = as.numeric(table(clustering)),
top_words = sapply(cluster_words, function(d){
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:5 ],
collapse = ", ")
}),
stringsAsFactors = FALSE)
View(cluster_summary)
tfidf <- t(eo_dtm[, tf_mat$term]) * tf_mat$idf # reweight terms by IDF
tfidf <- t(tfidf)
csim <- tfidf / sqrt(rowSums(tfidf * tfidf)) # calculate cosine similiarity
csim <- csim %*% t(csim)
cdist <- as.dist(1 - csim) #make it distance
hc <- hclust(cdist, "ward.D")
clustering <- cutree(hc, 80)
plot(hc, main = "Hierarchical clustering of Executive Orders",
ylab = "", xlab = "", yaxt = "n")
rect.hclust(hc, 80, border = "red")
# Cluster summary
p_words <- colSums(eo_dtm) / sum(eo_dtm)
cluster_words <- lapply(unique(clustering), function(x){
rows <- eo_dtm[ clustering == x , ]
# for memory's sake, drop all words that don't appear in the cluster
rows <- rows[ , colSums(rows) > 0 ]
colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})
cluster_summary <- data.frame(cluster = unique(clustering),
size = as.numeric(table(clustering)),
top_words = sapply(cluster_words, function(d){
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:5 ],
collapse = ", ")
}),
stringsAsFactors = FALSE)
View(cluster_summary)
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:20 ],
collapse = ", ")
cluster_summary <- data.frame(cluster = unique(clustering),
size = as.numeric(table(clustering)),
top_words = sapply(cluster_words, function(d){
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:20 ],
collapse = ", ")
}),
stringsAsFactors = FALSE)
View(cluster_summary)
tfidf <- t(eo_dtm[, tf_mat$term]) * tf_mat$idf # reweight terms by IDF
tfidf <- t(tfidf)
csim <- tfidf / sqrt(rowSums(tfidf * tfidf)) # calculate cosine similiarity
csim <- csim %*% t(csim)
cdist <- as.dist(1 - csim) #make it distance
hc <- hclust(cdist, "ward.D")
clustering <- cutree(hc, 150)
plot(hc, main = "Hierarchical clustering of Executive Orders",
ylab = "", xlab = "", yaxt = "n")
rect.hclust(hc, 150, border = "red")
# Cluster summary
p_words <- colSums(eo_dtm) / sum(eo_dtm)
cluster_words <- lapply(unique(clustering), function(x){
rows <- eo_dtm[ clustering == x , ]
# for memory's sake, drop all words that don't appear in the cluster
rows <- rows[ , colSums(rows) > 0 ]
colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})
cluster_summary <- data.frame(cluster = unique(clustering),
size = as.numeric(table(clustering)),
top_words = sapply(cluster_words, function(d){
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:10 ],
collapse = ", ")
}),
stringsAsFactors = FALSE)
View(cluster_summary)
####################
# Topic model
####################
set.seed(323)
eo_lda <- FitLdaModel(dtm = eo_dtm,
k = 80,
iterations = 1000,
burnin = 180,
alpha = 0.1,
beta = 0.05,
optimize_alpha = TRUE,
calc_likelihood = TRUE,
calc_coherence = TRUE,
calc_r2 = TRUE,
cpus = 2)
####################
# Interpret model
####################
plot(model$log_likelihood, type = "l")
####################
# Interpret model
####################
plot(eo_lda$log_likelihood, type = "l")
eo_lda$r2
hist(eo_lda$coherence)
head(t(eo$top_terms))
head(t(eo_lda$top_terms))
head(t(eo_lda$top_terms))
model$top_terms <- GetTopTerms(phi = model$phi, M = 10)
eo_lda$top_terms <- GetTopTerms(phi = eo_lda$phi, M = 10)
head(t(eo_lda$top_terms))
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 0.05,
dtm = eo_dtm,
M = 1)
eo_lda$labels
eo_lda$summary <- data.frame(topic = rownames(eo_lda$phi),
label = eo_lda$labels,
coherence = round(eo_lda$coherence, 3),
prevalence = round(eo_lda$prevalence,3),
top_terms = apply(eo_lda$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
# Summarise
eo_lda$prevalence <- colSums(eo_lda$theta) / sum(eo_lda$theta) * 100
eo_lda$summary <- data.frame(topic = rownames(eo_lda$phi),
label = eo_lda$labels,
coherence = round(eo_lda$coherence, 3),
prevalence = round(eo_lda$prevalence,3),
top_terms = apply(eo_lda$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
View(eo_lda$summary)
?FitLdaModel
####################
# Interpret model
####################
plot(eo_lda$log_likelihood, type = "l")
abline(v = 180)
mean(eo_lda$coherence)
mean(eo_lda$coherence * eo_lda$prevalence)
eo_lda$coherence * eo_lda$prevalence
eo_lda$prevalence
eo_lda$coherence
View(cluster_summary)
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 0.1,
dtm = eo_dtm,
M = 1)
eo_lda$summary <- data.frame(topic = rownames(eo_lda$phi),
label = eo_lda$labels,
coherence = round(eo_lda$coherence, 3),
prevalence = round(eo_lda$prevalence,3),
top_terms = apply(eo_lda$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
View(cluster_summary)
eo_lda$labels
View(eo_lda$summary)
eo_lda$theta
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 0.1,
dtm = eo_dtm,
M = 3)
eo_lda$summary <- data.frame(topic = rownames(eo_lda$phi),
label = eo_lda$labels,
coherence = round(eo_lda$coherence, 3),
prevalence = round(eo_lda$prevalence,3),
top_terms = apply(eo_lda$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
View(eo_lda$summary)
test <- eo_lda$summary
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 0.2,
dtm = eo_dtm,
M = 3)
# Summarise
eo_lda$prevalence <- colSums(eo_lda$theta) / sum(eo_lda$theta) * 100
eo_lda$summary <- data.frame(topic = rownames(eo_lda$phi),
label = eo_lda$labels,
coherence = round(eo_lda$coherence, 3),
prevalence = round(eo_lda$prevalence,3),
top_terms = apply(eo_lda$top_terms, 2, function(x){
paste(x, collapse = ", ")
}),
stringsAsFactors = FALSE)
View(eo_lda$summary)
eo_lda$labels <- LabelTopics(assignments = eo_lda$theta > 0,
dtm = eo_dtm,
M = 3)
# Save DTM
save(eo_dtm, "data_derived/eo_dtm.Rdata")
# Save DTM
save(eo_dtm, file = "data_derived/eo_dtm.Rdata")
source("scripts/util/__Util_MASTER.R")
library(parallel)
library(snowfall)
####################
# Load data
####################
load("data/eosCurated.RData")
##################################################################################################
#
# Topic model EOs
#
##################################################################################################
rm(list =ls())
rm(list =ls())
source("scripts/util/__Util_MASTER.R")
library(parallel)
library(snowfall)
####################
# Load data
####################
load("data/eosCurated.RData")
####################
# Topic model
####################
# Set topic numbers to test
Ks <- seq(10, 100, 5)
k = 85
# Save
file_name <- paste0("EO_lda_k", k)
file_name
# Save
file_name <- paste0("eo_lda_k", k)
file_name
Ks
####################
# Topic model
####################
# Set topic numbers to test
Ks <- seq(20, 110, 5)
Ks
rm(list =ls())
source("scripts/util/__Util_MASTER.R")
library(parallel)
library(snowfall)
stopwords('smart')
load("data_derived/dtms/eo_dtm.Rdata")
dim(eo_dt)
dim(eo_dtm)
hist(colSums(eo_dtm))
hist(colSums(eo_dtm), breaks = 1)
hist(colSums(eo_dtm), breaks = 1000)
word_freq <- colSums(eo_dtm)
word_freq <- log10(word_freq)
hist(word_freq)
table(word_freq)
# Basic corpus statistics
tf <- TermDocFreq(dtm = eo_dtm)
names(tf)
View(tf)
hist(tf$doc_freq)
load('data/eosCurated.RData')
tf$doc_freq_perc <- tf$doc_freq / nrow(eos)
jost)tf$doc_freq_perc
hist(tf$doc_freq_perc)
hist.data <- hist(tf$doc_freq_perc, plot = F)
hist.data$counts <- log10(hist.data$counts)
plot(hist.data)
qplot(hist.data)
str(hist.data)
hist(hist.data)
plot(hist.data)
tf$term[tf$doc_freq > 3000]
tf$term[tf$doc_freq > nrow(eos)/2]
tf$term[tf$doc_freq < nrow(eos)/100]
keep_terms <- tf$term[ tf$doc.freq <= nrow(dtm)/2 ]
keep_terms <- tf$term[ tf$doc.freq <= nrow(eo_dtm)/2 ]
keep_terms <- tf$term[ tf$doc_freq <= nrow(eo_dtm)/2 ]
eo_dtm <- eo_dtm[, keep_terms]
summary(rowSums(eo_dtm)) #check to make sure all documents have words
which(rowSums(eo_dtm == 2))
which(rowSums(eo_dtm) < 10)
which(rowSums(eo_dtm)== 2)
head(rownames(eo_dtm))
rownames(eo_dtm)[10002]
eos[eos$num == 10002]
eos$text[eos$num == 10002]
eos$text[ eos$word.count < 10]
eos$text[ eos$word.count < 20]
eos$text[ eos$word.count < 50]
eos$text[ eos$word.count < 10]
eos$text[ eos$word.count < 20]
rm(list = ls())
# Load packages, functions, and data
source("scripts/util/__Util_MASTER.R")
load("data/eosRaw.new.RData")
#remove a few FDR EOs that were scraped from the 1945 page
eos <- filter(eos, !(num %in% c("9511", "9523",
"9524", "9531",
"9533")))
#remove unnecessary html snippets
eos$html <- str_replace_all(string      = eos$html,
pattern     = "</div></div><span class=\"displaytext\">",
replacement = "")
eos$html <- str_replace_all(string      = eos$html,
pattern     = "</span><hr noshade=\"noshade\" size=\"1\"><span class=\"displaynotes\"><i></i></span><hr noshade=\"noshade\" size=\"1\">",
replacement = "")
#remove non-breaking spaces
eos$html <- str_replace_all(string      = eos$html,
pattern     = "&nbsp;",
replacement = " ")
#replace unicode apostrophes
eos$html <- str_replace_all(string      = eos$html,
pattern     = "\u0092",
replacement = "'")
#replace other unicode characters
eos$html <- iconv(x    = eos$html,
from = "utf-8",
to   = "ascii",
sub  = " ")
#remove html tags
eos$text <- str_replace_all(string      = eos$html,
pattern     = "<.*?>",
replacement = " ")
##############################
# Remove Unpublished EOs
##############################
#convert multiple spaces to single spaces
eos$text <- str_replace_all(string      = eos$text,
pattern     = "\\s+",
replacement = " ")
#rough word count
eos$word.count <- str_count(string  = eos$text,
pattern = " ") + 1
summary(eos$word.count)
shortEOs <- filter(eos, eos$word.count < 10)
unpublishedEOs <- shortEOs$num
shortEOs$text
# Find and remove unpublished EOs
shortEOs <- filter(eos, eos$word.count < 10)
unpublishedEOs <- shortEOs$num
eos <- filter(eos, word.count > 10)
summary(eos$word.count)
source('~/Documents/Research/Executive Orders/executive_orders/scripts/01-curateEOs.R', echo=TRUE)
source('~/Documents/Research/Executive Orders/executive_orders/scripts/01-curateEOs.R', echo=TRUE)
source('~/Documents/Research/Executive Orders/executive_orders/scripts/01-curateEOs.R', echo=TRUE)
pres
pres[1]
pres[2]
pres <- do.call('rbind', pres)
#remove letters to make EO matching simpler
eos$num2 <- str_replace_all(string      = eos$num,
pattern     = "-?[A-Z]$",
replacement = "")
eos <- merge(x     = eos,
y     = pres,
by    = "num2",
all.x = TRUE)
eos <- select(eos, -num2)
missingEOs <- merge(x     = missingEOs,
y     = pres,
by    = "num2",
all.x = TRUE)
#remove Trump EOs
eos <- eos[eos$president != "45-Trump", ]
source('~/Documents/Research/Executive Orders/executive_orders/scripts/01-curateEOs.R', echo=TRUE)
names(eos)
test <- eos %>% group_by(president) %>% summarise(freq = n())
test
plot(test)
plot(test$freq)
eos$num[grepl(eos$president, "obama")]
eos$num[grepl(eos$president, "Obama")]
eos$num[grepl("Obama", eos$president)]
rm(list =ls())
source("scripts/util/__Util_MASTER.R")
library(parallel)
library(snowfall)
####################
# Load data
####################
load("/scratch/gpfs/ctokita/ExecutiveOrders/data/eosCurated.RData")
load("data_derived/dtms/eo_dtm.Rdata")
# Remove overly frequent words
tf <- TermDocFreq(dtm = eo_dtm)
keep_terms <- tf$term[ tf$doc_freq <= nrow(eo_dtm)/2]
eo_dtm <- eo_dtm[, keep_terms]
summary(rowSums(eo_dtm)) #check to make sure all documents have words
View(tf)
test <- ("science", "scientific", "environment", "environmental")
test <- c("science", "scientific", "environment", "environmental")
library(SnowballC)
install.packages("corpus")
library(corpus)
text_tokens(test)
text_token(c("love", "loved"))
text_tokens(c("love", "loved"))
text_tokens(c("love", "loved"), stemmer = "en")
text_tokens(test, stemmer = "en")
hunspell::hunspell_stem("science")
hunspell::hunspell_stem("sciences")
hunspell::hunspell_stem("scienctific")
hunspell::hunspell_stem("environmental")
?CreateDtm
stopwords::stopwords(source = "en")
stopwords::stopwords(source = "smart")
